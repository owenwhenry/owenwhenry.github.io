---
layout: post
title: Avoiding Data Errors - Or, How Not to be the New York City Board of Elections
---

News broke last night that the NYC Board of Elections had a large “oops” - as 
they were working to release ballot totals for the recent Mayoral race, they 
included 135k “test” ballots, leading to erroneous reporting of candidate vote 
totals. 

https://twitter.com/BOENYC/status/1410064145064599554

Everyone noticed this almost immediately, because some less popular candidates 
saw jumps in their vote total of more than 600%. It didn’t pass the smell test 
for most analysts, nor for the Adams campaign which very quickly cast doubts on 
the reported totals. 

https://twitter.com/ericadamsfornyc/status/1410041139198152708

It’s great that the problem was identified by so many people so quickly; the 
public was able to quickly gain an understanding of the scope of the issue, 
and that understanding no doubt helped the Board of Elections find and remedy 
the problem. It’s less great that the problem occurred in the first place, 
especially at a moment when there’s been so much focus on the integrity of U.S. 
elections. 

But as anyone who works with data can tell you - mistakes like this are not 
uncommon! In any project dealing with data, I guarantee at some point you’ll run 
into an issue that can be traced back to some type of misunderstanding of how 
data is processed, or what the business rules are for interpreting it. There is
no data analyst who has not had their work picked apart because of bad data 
definition, and no engineer who has not been hosed by a dataset holding different 
values or counts than they were expecting. If you work with data professionally, 
these are teaching experiences that you learn from, but sometimes those lessons 
come too late for the project. It’s very hard to regain your credibility once 
it’s been damaged, as the NYC BOE can no doubt attest!

So, how do you identify problems in your data, BEFORE you release your work to 
others?

# Know (and Document) the Process

Data is not immaculately conceived - it must be gathered and entered into some 
kind of collection point, whether that collection point is a ballot, a 
spreadsheet, or a database. As part of that collection, there are always 
decisions that people make about how to interpret the information they’re 
receiving. For something like a ballot or a standardized test, a good example
is the rule that you have to fully darken the “bubble” showing your selection. 
It makes good sense (the scanners need the bubble to be dark enough for it to 
register), and it seems fairly obvious to most of us - but so do most data 
rules that you interact with frequently. 

Where problems tend to arise is the point at which you stop thinking about a 
data rule regularly. It’s great to have rules for your data like “we enter test 
data into the system in order to make sure everything is working, and then we 
purge it”, but that’s a data governance decision - it doesn’t always translate 
into an actual management process that’s carried out as expected, as the BoE so 
aptly demonstrated. This is partially because there’s a game of telephone that 
gets played across people and across time between the point at which you make a 
decision about your data, and the point at which that decision has to get carried 
out. As information gets transferred between people, it can suffer from 
information entropy - the message gets degraded, and people forget what was 
agreed upon.

The only safeguard against this is writing everything down, and making sure that 
everyone involves reads and understands that documentation. If you have data 
processing rules, you have to have them written down somewhere that everyone, 
from customer service to engineering to management, can understand. This is 
particularly important for engineers, analysts, and others who write code to 
remember - we often write comments or docstrings that help us remember what’s 
happening in our code, but doing so places information out of reach of other 
people involved in the process. If you have a process that data has to go 
through in your system, make sure that it’s written down in a place where others 
can easily access it. 

# Separate Testing and Production

The error made by the BoE was due to test data existing in a production system. 
Having test data that you run through a process you’ve built, whether that process 
is simple or complex, is always worthwhile. It allows you to have a standard set 
of inputs that translate to an expected set of outputs. If you don’t get those 
expected outputs during testing, you know your process has hit a snag that you 
need to find and fix. But the mistake the BoE made was to commingle test data in 
their real live results reporting system, and that’s a mistake that’s easy to 
avoid. 

Modern software engineering has solved this problem through the concept of tiers
, where you have a place where you make changes to a process, you have a different 
place where you test those changes, and separately from both you have your live 
system (or your production environment). There are a lot of benefits to this, 
but the biggest is that changes you might make to your test environment can be 
checked without making a single change to production. Paired with a standard set 
of data you use for testing, this allows you to very easily see what happens 
when you want to make a change to your process, and ensure that the changes 
you’re making aren’t going to break things. 

I have worked on a number of projects that will follow this process for code, 
but not for their data. I have also seen projects where you have one gigantic spreadsheet 
and there's not a plan for testing. Both are recipes for trouble. If you have a 
very big, very important system or process by which you’re manipulating or 
displaying data, it needs to be tiered regardless of the contextr

# Profile, Profile, Profile
The reason the BoE’s error was caught so quickly is that members of the public 
were looking at descriptive statistics for the results - that is, they were 
looking at numbers that described what was in the data and how it was changing. 
This is commonly done as part of data analysis, but it absolutely should be 
done as part of testing your data as well. 

Take, for example, the humble percent change. Any dataset that is being updated 
over time can be described in terms of the number of rows that existed in a 
dataset previously, and the number of rows that exist in that dataset after 
your change - or:

Current 
