---
layout: post
title: Always profile vote/voter data.
---

News broke last night that the NYC Board of Elections had a large “oops” - as
they were working to release ballot totals for the recent Mayoral race, [they
included 135k “test” ballots, leading to erroneous reporting of candidate vote
totals.](https://twitter.com/BOENYC/status/1410064145064599554)

Everyone noticed this almost immediately, because some less popular candidates
saw jumps in their vote total of more than 600%. It didn’t pass the smell test
for most analysts, nor for the [Adams campaign which very quickly cast doubts on
the reported totals.](https://twitter.com/ericadamsfornyc/status/1410041139198152708)

It’s great that the problem was identified by so many people so quickly; the
public was able to quickly gain an understanding of the scope of the issue,
and that understanding no doubt helped the Board of Elections find and remedy
the problem. It’s less great that the problem occurred in the first place,
especially at a moment when there’s been so much focus on the integrity of U.S.
elections.

My data education started on voter files produced by boards of elections, 
and while the NYC BoE may have screwed up particularly egregiously, election boards in
general have some problems. I was taught that I needed to watch their output like
a hawk - madness lay in every column, even after being picked up and standardized by a
voter file vendor that aggregated them across states. Dropping large numbers of
voters is something you hear about in the news now and again, but sometimes we'd
get files where entire counties had been dropped due to a bad update.

(This was complicated by the fact that we didn't do row-by-row updates for state
voter files - we just dropped the entire old version of a given state's file and
replaced it with the updated file.)

The way my boss at the time guarded against this was very simple. For each file,
he identified a standard count for each categorical variable (i.e. "how many counties
are in the state of Florida") and monitored that those counts remained stable
from version to verison.

In addition, he also monitored the count of voters in a given file. As a general rule,
excepting a purge, we almost always expected this count to go up - counts less than
the version before were treated with suspicion, and were a cause for immediate
in-depth quality control.

What my boss had set up was an excellent method of profiling his data set - born
of experience and frequent troubleshooting. It was quick and dirty, but he'd done
it consistently for years which allowed him to establish a baseline set of expectations
for every state's voter file. Based on that dataset, we were then able to 
monitor for changes that were unexpected.

It worked for years for us. I'm now setting up a similar system at my current job,
where we're taking the same idea and expanding it into automated tests. 

I'm not sure why the BoE didn't do this. Or why they haven't trained someone to 
do it. 
